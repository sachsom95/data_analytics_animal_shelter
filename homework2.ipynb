{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> HOMEWORK 2 </h1>\n",
    "<h2> NOTE : </h2>\n",
    "<h5> I have in this assignment added the assignment 1 notebook of data prepration as this will: <ul><li> add better context for next step \n",
    "    <li>make data prepration more streamline for both initial dataset and new dataset </li><li> make the full process more streamlined.</li></ul>I will explicitly mention from where the objectives of assignment will start</h5>\n",
    "<h3> Background Information on data </h3>\n",
    "<p>The dataset was dervived from an animal shelter in Austin, Texas. The dataset contains information regarding the animals that was brought to the shelter. The data includes final outcome of animal whether it was succesfully adopted/returned to owner or if animal died at the shelter. The outcome of 1 indicates death while 0 indicates that animal was adopted/returned</p>\n",
    "The main objective in this homework is to build and evaluvate models for the animal shelter dataset. Find out how descriptive features are related to final outcome and to see if we can improve the model. The final predicition is <b>wheather an animal in the end have a positive or negative outcome.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1 Data Prepration and Cleaning</h2>\n",
    "    <p>Objective</p>\n",
    "    <ol><li>Import all required libraries</li>\n",
    "    <li>Find initial dimension of data</li>\n",
    "        <li>Print the first and the last 5 rows.</li>\n",
    "        <li>Convert the features to their appropriate data types </li>\n",
    "        <li>Drop duplicate rows and columns, if any.</li>\n",
    "        <li>Save your updated/cleaned data frame to a new csv file.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.Import libraries</h3>\n",
    "<p> The most important library used is the <b>pandas</b>, this enables the csv to be converted to a pandas datatype and allow data manipulation. <b>Numpy</b> library adds array like functionalities to python for scientific calculations. The visulaization is carried out using <b>Matplotlib</b> and <b>Seaborn</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This option enables auto complete in notebook\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Adding the new test data as well</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv to pandas dataframe\n",
    "df = pd.read_csv('AnimalWelfareRisk-19200494.csv')\n",
    "df_test = pd.read_csv('28032020-AnimalWelfareRisk-binaryOutcome-recent-1k.csv')\n",
    "# Enables viewing of all columns when using the head and tail command\n",
    "pd.options.display.max_columns = None\n",
    "# pd.options.display.max_rows = None\n",
    "\n",
    "\n",
    "# Show full column with out truncation\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.Get initial dimension of dataframe</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df shape:\",df.shape)\n",
    "print(\"df_test shape:\",df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>The dataframe initially has dimension of 1000 x 22 which means there are 22 columns of attributes and 1000 rows of data</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.Getting an overview of data </h3>\n",
    "<p>by checking first 5 and last 5 data by using head() and tail() functions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Checking datatypes of all attributes</h3>\n",
    "Almost all the data is of  object type with only final output being a float type\n",
    "need more exploration to be done to check if all data is to be kept as object or any other types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Since most of the datatypes is of type object we will not get much information by describing data</p> The data types of each attribute will be analyzed and converted in sections below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T\n",
    "# Such results not usable due to data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe().T\n",
    "# Such results not usable due to data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Finding duplicate columns </h3>\n",
    "<p>\n",
    "A function called <b>find_duplicates(dataframe)</b> is used to find all duplicate columns in dataframe (which doesn't have null values).\n",
    "<br>The function accepts a dataframe and goes through each column while comparing the contents with other columns<br>If a column is found to have same content with the compared column. The column ids are added as pair to a duplicate list and returned in the end</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(dataframe):\n",
    "    x = dataframe.columns\n",
    "    y = dataframe.columns\n",
    "    row_size = dataframe.shape[0]\n",
    "    duplicate_pairs = []\n",
    "    for index1 in range (len(x)):\n",
    "        for index2 in range (index1+1,len(y)):\n",
    "            if sum(df[x[index1]]==df[y[index2]])==row_size:\n",
    "                duplicate_pairs.append([x[index1],y[index2]])\n",
    "    return duplicate_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_pairs=find_duplicates(df)\n",
    "print(\"The duplicate pairs are :\\n\")\n",
    "for x,y in  duplicate_pairs:\n",
    "    print (\" {} --> duplicate of column --> {}\\n\".format(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_pairs=find_duplicates(df_test)\n",
    "print(\"The duplicate pairs are :\\n\")\n",
    "for x,y in  duplicate_pairs:\n",
    "    print (\" {} --> duplicate of column --> {}\\n\".format(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>Observations</blockquote>\n",
    "By executing the function to find duplicates its observed that there are 5 pairs of duplicate columns in dataframe. An important thing to note is that the function only checks for columns which have all values. Therefor more investigation is to be carried out to see if there are columns which have duplicates and null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Finding out all the null values in attributes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>Observation</blockquote>\n",
    "It seems that 'Name_Intake' and 'Name_outcome' are the only columns where there seems to be missing information. Around 35.2 % of data is missing. Another interesting observation is that the exact amount of data is also missing. This means thses two might be duplicates of each other and since the find_duplicate function only checks functions which have full information. We will have to check these two individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['Name_Intake']==df['Name_Outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote> Observation </blockquote>Here upon checking columns 'Name_intake' and 'Name_Outcome' both seems to have same content for all 648 available entries\n",
    "the rest 352 are null , so we can say by sure most of elemnts are duplicates of each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7. Dropping columns </h3>\n",
    "<p>Most real life datasets contains some form of errors , unnecessary attributes or missing information. In such cases the dataset must be cleaned before it is used inside a model for training. The same case is applicable in this scenario as well. In the given dataset there seems to be duplicate columns, missing data and redundant information. Therefor it is important that we drop these unnecessary columns. Detailed explaination for droping them will be provided below<p>\n",
    "    This will be done for both the datasets loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The most obvious columns that will be dropped are the duplicate ones as holding on to redundant data is of no value. so columns <br>\n",
    "<ul><li>MonthYear_Intake</li>\n",
    "    <li> Animal Type_Intake</li>\n",
    "    <li>Breed_Intake </li>\n",
    "    <li>Color_Intake</li>\n",
    "    <li>MonthYear_Outcome</li>\n",
    "<br>Will be dropped from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"MonthYear_Intake\",\"Animal Type_Intake\",\"Breed_Intake\",\"Color_Intake\",\"MonthYear_Outcome\"],inplace = True , axis=1)\n",
    "df_test.drop(columns=[\"MonthYear_Intake\",\"Animal Type_Intake\",\"Breed_Intake\",\"Color_Intake\",\"MonthYear_Outcome\"],inplace = True , axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After droping the colums of new dataframe is checked using df.head(1). And it shows that all duplicates where dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next attibutes we look at are the \"Name_Intake\" and \"Name_Outcome\" we look at it because of <b>three</b> reasons<br>\n",
    "<ul> <li>Almost 35.2% of the data is missing in both the attributes.</li>\n",
    "        <li> Of the available 64.8% data both seems to share exact data. Therefor it seems both are duplicate of each others. Imputation is not possible in this data as these are names and we cannot use statistical inferance to impute data.</li>\n",
    "    <li>Since these data are just names the attibutes hold no meaning to overall analysis. </li></ul>\n",
    "<p> Considering all these factors its best to remove both the attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Name_Intake\",\"Name_Outcome\"],inplace = True , axis=1)\n",
    "df_test.drop(columns=[\"Name_Intake\",\"Name_Outcome\"],inplace = True , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Another attribute which can be removed is the attribute \"Animal ID\" because it doesn't provide any information</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Animal ID\"],inplace = True , axis=1)\n",
    "df_test.drop(columns=[\"Animal ID\"],inplace = True , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The column date of birth can be used to find age of an animal, However, There is already a column called 'Age upon outcome' which will the same result. Therfore column 'date of birth' can be removed as same data can be derived from other columns as well</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Date of Birth\"],inplace = True , axis=1)\n",
    "df_test.drop(columns=[\"Date of Birth\"],inplace = True , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8. Data Types</h3>\n",
    "<p> Since we have cleaned up most of the data of null values and redundancies. The focus should be for now shifted towards the datatypes of attributes. By looking at the dataframe inspected in section 4 (above) we found out that almost all data are of type object and only the final outcome is of type float. For a proper exploratory analysis to be done the datatypes for attibutes must be a proper one. The most common of the two are categorical and continious type.</p>\n",
    "\n",
    "<p>First thing to check are the cardinalities of the attributes. Any attributes with cardiality of 1 must be investigated as it will not be able to provide usefull information.</p> <br> Therefor we investigate the cardinalities of each attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cardinality(dataframe):\n",
    "    list_colmn = []\n",
    "    list_data = []\n",
    "    print(\"The cardinalities of all the columns are\\n\")\n",
    "    for x in dataframe.columns:\n",
    "        list_colmn.append(x)\n",
    "        list_data.append(dataframe[x].nunique())\n",
    "    data = {'Attributes':list_colmn, 'Cardinalities':list_data}\n",
    "    display(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cardinality(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<blockquote>Observation<br><p> From the cardinalities printed we can see that there are no attributes with a cardinality of just 1; Another observation is the fact that there are some attributes with cardinalities less than the number of instances.Attribute which cannot be used for statistical analysis like mean , mode , median etc are generally good candidates for Categorical datatypes.</p></blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> First we will try to find all the low cardinality attributes which have cardinalities less than 10 and analyse them first</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_elements_by_cardinality(dataframe,cardinality):\n",
    "    \n",
    "    low_cardinality_list = dataframe.columns[dataframe[dataframe.columns].nunique() < cardinality ]\n",
    "    low_cardinality_list_info =dataframe[low_cardinality_list].nunique()\n",
    "    data ={\"Cardinality\":low_cardinality_list_info}\n",
    "    display(pd.DataFrame(data))\n",
    "    return(low_cardinality_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cardinality_columns = filter_elements_by_cardinality(df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<blockquote>Observation<br><p> By filtering out first attributes with Cardinality less than 10. We discovered 6 attributes that fit the category.</p></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_unique_elements(df,columns):\n",
    "    list = []\n",
    "    list_index = []\n",
    "    for x in columns:\n",
    "        list.append(df[x].unique())\n",
    "        list_index.append(x)\n",
    "    data = {\"Atributes\":list_index,\"Unique elements\":list}\n",
    "    display(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unique_elements(df,low_cardinality_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li>For the given attributes the cardinality of the attributes are much less than number of instances in dataset</li>\n",
    "    <li>The observed data is in no way continious</li>\n",
    "    <li> Fields like 'binary_outcome' and all the above attributes have fixed categories</li>\n",
    "    <li> No meaning full data can be extracted from mean of say 'intake condition' and other attributes hence another reason to consider categorical type</li>\n",
    "    <blockquote> Considering all the above factors its concluded that all the above attributes are to be converted to categorical type</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in low_cardinality_columns:\n",
    "    df[x]=df[x].astype('category')\n",
    "    df_test[x]=df_test[x].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>9.Data Transformation </h3>\n",
    "<p>Though this is being asked to do in later section i feel this must be done here itself<br>\n",
    "Some data such as 'Date Time' , 'Found location' , 'Age upon intake' , 'Date Time_outcome' , 'age upon outcome' are of type objects (String) and must be processed to extract usefull information and remove unnecessary ones so that we can use them for exploratory analysis</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attribute which will be processed to get usefull information will be 'DateTime_intake' to extract wheeter the animal came in the morning or at night. The intuition behind this is to find if there is a trend in animal abandoned at morning more or at night. This data if usefull can be used to identify peak times and allocate more resource at that time in shelter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AM_PM(df):\n",
    "    list =[]\n",
    "    for x in range (1000):\n",
    "        z = df['DateTime_Intake'][x].split()\n",
    "        list.append(z[2])\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_AM_PM(df)\n",
    "df['intake_am_pm'] = x\n",
    "# Convert it to category as well\n",
    "df['intake_am_pm'] = df['intake_am_pm'].astype('category');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_AM_PM(df_test)\n",
    "df_test['intake_am_pm'] = x\n",
    "# Convert it to category as well\n",
    "df_test['intake_am_pm'] = df_test['intake_am_pm'].astype('category');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract just the date. Since the date follows a definite pattern regular expression is used to extract the data. The data is then converted to date time format so that we can do exploration on it like what year most animals where brought and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extr = df['DateTime_Intake'].str.extract(r'(\\d{2}([/])\\d{1,2}([/])\\d{1,4})', expand=False)\n",
    "df['intake_time'] = extr[0]\n",
    "df['intake_time'] = pd.to_datetime(df['intake_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extr = df_test['DateTime_Intake'].str.extract(r'(\\d{2}([/])\\d{1,2}([/])\\d{1,4})', expand=False)\n",
    "df_test['intake_time'] = extr[0]\n",
    "df_test['intake_time'] = pd.to_datetime(df['intake_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have extracted usefull information from the 'DateTime_Intake' attibutes we can now drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['DateTime_Intake'],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(['DateTime_Intake'],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next attribute to get feature extraction is<b> 'Found Location'</b> we take try to extract from where most of the animals are abandoned. The reason for this process is because if we check the attribute we initially find that the cardinality of the data is around 759 out of 1000. Which indicates that the data might to too unique to be used to find any relations. Observing the data one by one its found that the data is to specific ie almost all data contains data upto the street in which the animal is discovered. Such high granularity is not required instead we extract the major region where each animal is taken from which gives better information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1000):\n",
    "    z = df['Found Location'][x].split(' in')\n",
    "    if len(z) == 1:\n",
    "        df['Found Location'][x] = z[0].strip()\n",
    "    else:\n",
    "        df['Found Location'][x] = z[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1000):\n",
    "    z = df_test['Found Location'][x].split(' in')\n",
    "    if len(z) == 1:\n",
    "        df_test['Found Location'][x] = z[0].strip()\n",
    "    else:\n",
    "        df_test['Found Location'][x] = z[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting all the general location. I was able to remove the high granularity of 'Found location' data from 756 unique values to 16 unique locations which provides more information to the attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of unique locations is: \" + str(df['Found Location'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = df['Found Location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the major location and animals picked from the place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have categorized the found location data its time for us to convert the datatype from object type to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Found Location']=df['Found Location'].astype('category');\n",
    "df_test['Found Location']=df_test['Found Location'].astype('category');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we observe is that there are still some attributes with datatype as '<b>object</b>' these are :\n",
    "    <ul><li>Age upon Intake</li>\n",
    "    <li>Breed_Outcome</li>\n",
    "    <li>Color_Outcome</li>\n",
    "    <li>DateTime_Outcome</li>\n",
    "    <li>Age upon Outcome</li></ul>\n",
    "Further investigation into thses attributes will be carried out below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colums = ['Age upon Intake','Breed_Outcome','Color_Outcome','DateTime_Outcome','Age upon Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these attributes a number of problems are immediately apparent.\n",
    "<ul>\n",
    "    <li>The features 'Age upon Intake' and and 'Age upon Outcome' both express time period however non of them follow a common unit. Corrective measures such as converting to a single unit will be needed to make it viable data</li>\n",
    "    <li> The attribute 'Breed_Outcome' seems to have species and animal types mixed example there is both dog breeds and animal type like bats , chicken etc in same table which makes it difficult to seggregate. Another issue is the number of varieties of varieous species. Example in case of dogs. This posses a problem on how we can segregate species to animals , and wherether we can use this attribute for analysis</li>\n",
    "    <li> Similar to the above attributes 'Color Outcome' attribute seems to have a high granularity and also seems to have multiple values for some animals as many animals have multiple colors; How to segregate or extract information from this is also need to be investigated</li>\n",
    "    <li>The final attribute 'DateTime_Outcome' attribute indicates when the animal came out from shelter. However its my observation that this attribute gives same information that can be derived from attributes 'Age upon Intake' and 'Age upon Outcome'. Therfore this is a redundant attribute hence will be dropped</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping DateTime_outcome from table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['DateTime_Outcome'],inplace=True,axis=1)\n",
    "df_test.drop(columns=['DateTime_Outcome'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the attribute 'Breed_Outtake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Breed_Outcome'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df['Breed_Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above table it seems that the attibute can provide some usefull insights. It seems that there are large number of animals who has value count of around 1. These mostly belong to other category in animal type and are usually rare animals. So what we will do is that all animals of type others and value count is low will be put in category rare. Dogs and cat species who have low value count will be put in others category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before that I will bucket all the mix and normal breeds together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mix(dataframe):\n",
    "    for x in range (1000):\n",
    "        txt = df['Breed_Outcome'][x]\n",
    "        print(txt)\n",
    "        x = re.sub(\"Mix\", \"\", txt)\n",
    "        print(x)\n",
    "        df['Breed_Outcome'][x] = x\n",
    "        print(df['Breed_Outcome'][x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df[(df[\"binary_outcome\"] == 1)]\n",
    "x_0=df[(df[\"binary_outcome\"] == 0)]\n",
    "y=x[x[\"Animal Type_Outcome\"]== \"Other\"]\n",
    "z=y[y[\"binary_outcome\"]==1]\n",
    "z[\"Breed_Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the attribute 'Breed_intake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Breed_Outcome'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df['Breed_Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pd.DataFrame(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x=df[(df[\"binary_outcome\"] == 1)]\n",
    "x_0=df[(df[\"binary_outcome\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"Animal Type_Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"Color_Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0[\"Color_Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x[x[\"Animal Type_Outcome\"]== \"Dog\"]\n",
    "z=y[y[\"binary_outcome\"]==1]\n",
    "z[\"Age upon Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x[x[\"Animal Type_Outcome\"]== \"Dog\"]\n",
    "z=y[y[\"binary_outcome\"]==1]\n",
    "z[\"Breed_Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x[x[\"Animal Type_Outcome\"]== \"Other\"]\n",
    "z=y[y[\"binary_outcome\"]==1]\n",
    "z[\"Breed_Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x_0[x_0[\"Animal Type_Outcome\"]== \"Cat\"]\n",
    "z=y[y[\"binary_outcome\"]==1]\n",
    "z[\"Breed_Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START  OF ASSIGNMENT 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1 Data Understanding and Preperation</h2>\n",
    "    <p>Objective</p>\n",
    "    <ul><li>Import all required libraries</li>\n",
    "    <li> Import the cleaned csv file and review it\n",
    "    <li>Explore relationship between feature pairs and transform them to meet requirements of training</li>\n",
    "        <li>Shuffle data and split them into test and train data.</li>\n",
    "    </ul>\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Import libraries</h3>\n",
    "<p> The most important library used is the <b>pandas</b>, this enables the csv to be converted to a pandas datatype and allow data manipulation. <b>Numpy</b> library adds array like functionalities to python for scientific calculations. The visulaization is carried out using <b>Matplotlib</b> and <b>Seaborn</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done to ignore warnings generated by Jupyter Notebook and improve readablity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is the package that will be used to train the various models. Its an open source Ml library which can be used for regression , classfication, random forest , spliting of data and cross-validation. It is designed to work well with numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph viz will be used here to visualize trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This option enables auto complete in notebook\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv to pandas dataframe\n",
    "df = pd.read_csv('AnimalWelfareRisk-19200494.csv')\n",
    "# Enables viewing of all columns when using the head and tail command\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "# Show full column with out truncation\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The data that is going to be used in this homework was already prepared and cleaned in homework 1 </p>\n",
    "<ul>\n",
    "    <li>Additional information on the prepared data can be found in dataquality report provided along with the homework</li>\n",
    "    <li>A pdf version of homework 1 notebook is also provided </li>\n",
    "    <li>Summary of data quality report can be viewed in table below</li>\n",
    "    </ul>\n",
    "    \n",
    " <h3>Summary of data quality plan:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ATTRIBUTE \t| DATA QUALITY ISSUE \t| HANDLING STRATEGY \t|\n",
    "|---------------------\t|:------------------------------------------------------------------------------------------------:\t|----------------------------------------------------------------------------------------\t|\n",
    "| Animal ID \t| Data quality satisfactory. No useful information provide \t| Dropped \t|\n",
    "| Name_Intake \t| Some values missing (NaN). Imputations  not possible.No useful information provided. \t| Dropped \t|\n",
    "| DateTime_Intake \t| Data quality satisfactory. High cardinality. \t| Time data removed, Additional data extracted (AM/PM) and date \t|\n",
    "| MonthYear_Intake \t| Duplicate of column 'DateTime_Intake' \t| Dropped \t|\n",
    "| Found Location \t| Data satisfactory , High cardinality \t| Street wise data removed and only  city information retained. \t|\n",
    "| Intake Type \t| Data quality satisfactory \t| No action required \t|\n",
    "| Intake Condition \t| Data quality satisfactory \t| No action required \t|\n",
    "| Animal Type_Intake \t| Data quality satisfactory \t| No action required \t|\n",
    "| Sex upon Intake \t| Data quality satisfactory \t| No action required \t|\n",
    "| Age upon Intake \t| Data quality satisfactory, Multiple units used \t| Convert all data to single unit of time \t|\n",
    "| Breed_Intake \t| Data quality satisfactory, High cardinality \t| Data with less frequency binned to others category \t|\n",
    "| Color_Intake \t| High cardinality, ambiguous information, Same data with different name. \t| Dropped \t|\n",
    "| Name_Outcome \t| Duplicate of column 'Name_Intake'.Some data missing \t| Dropped \t|\n",
    "| DateTime_Outcome \t| Data inconsistency , some dates lower than possible \t| Dates which show inconsistency imputed to same value of date in column DateTime_Intake \t|\n",
    "| MonthYear_Outcome \t| Duplicate of column 'DateTime_Outcome' \t| Dropped \t|\n",
    "| Date of Birth \t| Data quality satisfactory; However same info extractable  from other columns.Therefor redundant. \t| Dropped \t|\n",
    "| Animal Type_Outcome \t| Duplicate of column 'Animal Type_Intake' \t| Dropped \t|\n",
    "| Sex upon Outcome \t| Data quality satisfactory , same as column 'Sex upon Intake' \t| Dropped \t|\n",
    "| Age upon Outcome \t| Data quality satisfactory \t| No action required \t|\n",
    "| Breed_Outcome \t| Duplicate of colum \"Breed Intake' \t| Dropped \t|\n",
    "| Color_Outcome \t| High cardinality, ambiguous information , Same data with different name. \t| Dropped \t|\n",
    "| binary_outcome \t| Wrong datatype of float for type binary outcome \t| Convert from Float to Categorical \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Apart from above mentioned steps taken. Three new features where added by combining the attributes. This was done to better capture the problem domain</p>\n",
    "They are:\n",
    "<ul>\n",
    "    <li>Seasons</li>\n",
    "    <li>Time spend in shelter </li>\n",
    "    <li>Domestic factor</li>\n",
    "</ul>\n",
    "The explanation for each new attribute is present in pdf of homework 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.1 Review, prepare and split the dataset into two datasets: 70% training and 30% test\n",
    "Since we have presented the data quality report. We will now import the cleaned csv file inspect it and see if there are still any defects present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv \n",
    "# parameters used in read_csv\n",
    "# keep_default_na: Whether or not to include the default NaN values when parsing the data\n",
    "# sep : Delimiter to use \n",
    "# skipinitialspace : Skip spaces after delimiter.\n",
    "df = pd.read_csv(\"final.csv\", keep_default_na=True, sep=',\\s+', delimiter=',', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at data in brief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Dropping Columns</h4>\n",
    "<p>Attributes <b>index</b> and <b>percent</b> were used for assignment 1 analysis and is not required here. Therfore they will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"percent\",inplace = True , axis=1)\n",
    "df.drop(columns=\"index\",inplace = True , axis=1)\n",
    "df.drop(columns=\"Sex upon Intake\",inplace = True , axis=1)\n",
    "df.drop(columns=\"intake_time\",inplace = True , axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df.drop(columns=[\"Age upon Outcome\",\"Age upon Intake\",\"intake_am_pm\",],inplace = True , axis=1)\n",
    "\n",
    "\n",
    "# df.drop(columns=\"domesticIndex\",inplace = True , axis=1)\n",
    "# df.drop(columns=\"season\",inplace = True , axis=1)\n",
    "# df.drop(columns=\"time_spend_shelter\",inplace = True , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8. Data Types</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Checking datatypes of all attributes</h3>\n",
    "Almost all the data is of  object type with only final output being a float type\n",
    "need more exploration to be done to check if all data is to be kept as object or any other types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote> The csv is incapable of holding datatype information such as categorical type as it is an object in memory therfore again we have to convert features to required datatypes here </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T\n",
    "# Such results not usable due to data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features\n",
    "<ul>\n",
    "    <li>Intake Type</li>\n",
    "    <li>Intake Condition</li>\n",
    "    <li>Animal Type_Intake</li>\n",
    "    <li>Sex upon Outcome </li>\n",
    "    <li>binary outcome </li>\n",
    "    <li>intake_am_pm </li>\n",
    "    <li>Found Location</li>\n",
    "    <li>Breed_Intake</li>\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "</ul>\n",
    "A detailed explaination was provided in previous assignment \n",
    "\n",
    "Features\n",
    "<ul>\n",
    "    <li>Age upon Outcome</li>\n",
    "    <li>Age upon Intake</li>    \n",
    "</ul>\n",
    "are converted to type int\n",
    "\n",
    "\n",
    "Features\n",
    "<ul>\n",
    "    <li>intake_time</li>\n",
    "</ul>\n",
    "are converted to type int\n",
    "    data[\"time_spend_shelter\"] = data[\"time_spend_shelter\"].dt.days.astype('int16')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Intake Type\"]=df[\"Intake Type\"].astype('category')\n",
    "df[\"Intake Condition\"]=df[\"Intake Condition\"].astype('category')\n",
    "df[\"Animal Type_Intake\"]=df[\"Animal Type_Intake\"].astype('category')\n",
    "df[\"Sex upon Outcome\"]=df[\"Sex upon Outcome\"].astype('category')\n",
    "df[\"binary_outcome\"]=df[\"binary_outcome\"].astype('category')\n",
    "# df[\"intake_am_pm\"]=df[\"intake_am_pm\"].astype('category')\n",
    "df[\"Found Location\"]=df[\"Found Location\"].astype('category')\n",
    "df[\"Breed_Intake\"]=df[\"Breed_Intake\"].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"season\"]=df[\"season\"].astype('category')\n",
    "df[\"time_spend_shelter\"]=df[\"time_spend_shelter\"].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Finding out all the null values in attributes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>Observation</blockquote>\n",
    "It seems that there is no NaN values present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Setting up dummy features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dummies features\n",
    "df_rev1 = pd.get_dummies(df)\n",
    "df_rev1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding of target feature doesn't make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev1.drop(columns=[\"binary_outcome_0.0\", \"binary_outcome_1.0\"],inplace = True , axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is the target\n",
    "y = df[\"binary_outcome\"]\n",
    "# X is everything else\n",
    "X = df_rev1\n",
    "\n",
    "# Split the dataset into two datasets: 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,  test_size=0.3)\n",
    "\n",
    "print(\"original range is: \",df_rev1.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(X_train.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(X_train.shape[0]), \"to\", round(X_train.shape[0]) + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights learned for each feature.\n",
    "print(\"\\nFeatures are: \\n\", X_train.columns)\n",
    "print(\"\\nCoeficients are: \\n\", multiple_linreg.coef_)\n",
    "print(\"\\nIntercept is: \\n\", multiple_linreg.intercept_)\n",
    "print(\"\\nFeatures and coeficients: \\n\", list(zip(X_train.columns, multiple_linreg.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_linreg_predictions_train = (multiple_linreg.predict(X_train) >= 0.5) * 1.0\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_train, pd.DataFrame(multiple_linreg_predictions_train, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_linreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_linreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_linreg_predictions_train))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_linreg_predictions_test = (multiple_linreg.predict(X_test) >= 0.5) * 1.0\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([y_test, pd.DataFrame(multiple_linreg_predictions_test, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelinreg.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_linreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_linreg_predictions_test))\n",
    "print(\"Classification report - Test data:\\n \", metrics.classification_report(y_test, multiple_linreg_predictions_test))\n",
    "print(\"\\n==================== Train Data ======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_linreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_linreg_predictions_train))\n",
    "print(\"\\nClassification report: - Training data\\n \", metrics.classification_report(y_train, multiple_linreg_predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_LinReg(X, y, cv=3, scoring='accuracy'):\n",
    "    \"\"\"Functions to carry out cross validation on the linear regression model\n",
    "    Default number of validations is 3. The randon state will be updated \n",
    "    at each iteration to allow our results to be repeated\"\"\"\n",
    "    \n",
    "    # store results\n",
    "    results = []\n",
    "    # evaluate cv times and append to results\n",
    "    for i in range(cv):\n",
    "        # set up train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=i , test_size=0.3)\n",
    "        # generate model\n",
    "        multiple_linreg = LinearRegression().fit(X_train, y_train)\n",
    "        # threshold\n",
    "        multiple_linreg_predictions = (multiple_linreg.predict(X_test) >= 0.5) * 1.0\n",
    "        # calc score\n",
    "        if scoring=='accuracy':\n",
    "            score = metrics.accuracy_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='precision':\n",
    "            score = metrics.precision_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='f1':\n",
    "            score = metrics.f1_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='recall':\n",
    "            score = metrics.recall_score(y_test, multiple_linreg_predictions)\n",
    "        # append to results\n",
    "        results.append(score)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_LinReg_DF(X,y):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    linRegResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1']\n",
    "    \n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_LinReg(X, y, cv=10, scoring=metric)\n",
    "        length = len(result)\n",
    "        # store result in dict\n",
    "        linRegResults[metric] = sum(result)/length\n",
    "\n",
    "    # create dataframe with results\n",
    "    LinRegDF = pd.DataFrame.from_dict(linRegResults, orient='index', columns=['Linear_Regression'])\n",
    "    \n",
    "    return LinRegDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linRegDF = cross_val_LinReg_DF(X,y)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "linRegDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_logisticreg = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights learned for each feature.\n",
    "print(\"\\nFeatures are: \\n\", X_train.columns)\n",
    "print(\"\\nCoeficients are: \\n\", multiple_logisticreg.coef_[0])\n",
    "print(\"\\nIntercept is: \\n\", multiple_logisticreg.intercept_)\n",
    "print(\"\\nFeatures and coeficients: \\n\", list(zip(X_train.columns, multiple_logisticreg.coef_[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_logisticreg_predictions_train = multiple_logisticreg.predict(X_train)\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelogisticreg = pd.concat([y_train, pd.DataFrame(multiple_logisticreg_predictions_train, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelogisticreg.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_logisticreg_predictions_test = multiple_logisticreg.predict(X_test)\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelogisticreg = pd.concat([y_test, pd.DataFrame(multiple_logisticreg_predictions_test, columns=['Predicted'])], axis=1)\n",
    "print(actual_vs_predicted_multiplelogisticreg.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, multiple_logisticreg_predictions_test))\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, multiple_logisticreg_predictions_train))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_LogReg_DF(X,y):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    logRegResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1']\n",
    "\n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_score(LogisticRegression(), X, y, scoring=metric, cv=10)\n",
    "        # store result in dict\n",
    "        logRegResults[metric] = result.mean()\n",
    "        \n",
    "    # create dataframe with results\n",
    "    LogRegDF = pd.DataFrame.from_dict(logRegResults, orient='index', columns=['Logistic_Regression'])\n",
    "    \n",
    "    return LogRegDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegDF = cross_val_LogReg_DF(X,y)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "logRegDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RF with 100 trees\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc4 = DecisionTreeClassifier(max_depth=4, random_state=1)\n",
    "dtc10 = DecisionTreeClassifier(max_depth=10, random_state=1)\n",
    "dtc4.fit(X_train, y_train)\n",
    "dtc10.fit(X_train, y_train)\n",
    "print(\"Max depth 4: \\n\",dtc4)\n",
    "print(\"Max depth 10: \\n\",dtc10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Graphviz png\n",
    "with open(\"DecisionTree4.dot\", 'w') as f1:\n",
    "    f1 = export_graphviz(dtc4, out_file=f1, feature_names=X_train.columns)\n",
    "with open(\"DecisionTree10.dot\", 'w') as f2:\n",
    "    f2 = export_graphviz(dtc10, out_file=f2, feature_names=X_train.columns)\n",
    "!dot -Tpng DecisionTree4.dot -o DecisionTree4.png\n",
    "!dot -Tpng DecisionTree10.dot -o DecisionTree10.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'feature': X_train.columns, 'importance':rfc.feature_importances_})\n",
    "importance.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class labels for all examples, \n",
    "# using the trained model, on in-sample data (same sample used for training and test)\n",
    "rfc_predictions_train = rfc.predict(X_train)\n",
    "df_true_vs_rfc_predicted = pd.DataFrame({'ActualClass': y_train, 'PredictedClass': rfc_predictions_train})\n",
    "df_true_vs_rfc_predicted.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, rfc_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, rfc_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, rfc_predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class labels for all examples, \n",
    "# using the trained model, on in-sample data (same sample used for training and test)\n",
    "rfc_predictions_test = rfc.predict(X_test)\n",
    "df_true_vs_rfc_predicted_test = pd.DataFrame({'ActualClass': y_test, 'PredictedClass': rfc_predictions_test})\n",
    "df_true_vs_rfc_predicted_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== Test Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, rfc_predictions_test))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_test, rfc_predictions_test))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_test, rfc_predictions_test))\n",
    "print(\"==================== Train Data =======================\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train, rfc_predictions_train))\n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train, rfc_predictions_train))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train, rfc_predictions_train))\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_RandomForest_DF(X,y, depth=None, estimators=100):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    # store results in dict\n",
    "    RandomForestResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1']\n",
    "\n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_score(RandomForestClassifier(n_estimators=estimators, max_features='auto', oob_score=True, random_state=1, max_depth=depth), X, y, scoring=metric, cv=10)\n",
    "        # store result in dict\n",
    "        RandomForestResults[metric] = result.mean()\n",
    "    \n",
    "    # create dataframe with results\n",
    "    RandomForestDF = pd.DataFrame.from_dict(RandomForestResults, orient='index', columns=['Random_Forests'])\n",
    "\n",
    "    return RandomForestDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestDF = cross_val_RandomForest_DF(X,y)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "RandomForestDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the out-of-bag classification accuracy\n",
    "rfc.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsDF = pd.concat([linRegDF, logRegDF, RandomForestDF], axis=1)\n",
    "ResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dummies features\n",
    "df_rev1 = pd.get_dummies(df)\n",
    "df_rev1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataframe\n",
    "df_all = pd.get_dummies(df)\n",
    "y = df[\"binary_outcome\"]\n",
    "X = df_all.drop([\"binary_outcome_1.0\",\"binary_outcome_0.0\"],1)\n",
    "\n",
    "# initialised empty daraframe\n",
    "RandomForest_Depth = pd.DataFrame()\n",
    "\n",
    "# calculate cross val score incrementing max depth by 1 each iteration\n",
    "# append results to dataframe\n",
    "for i in range(1,11):\n",
    "    df1 = cross_val_RandomForest_DF(X,y,i)\n",
    "    df1.rename(columns={'Random_Forests':f'depth={i}'}, inplace=True)\n",
    "    RandomForest_Depth = pd.concat([RandomForest_Depth, df1], axis=1)\n",
    "\n",
    "# same calculation but use no max depth this time\n",
    "no_max_depth = cross_val_RandomForest_DF(X,y)\n",
    "no_max_depth.rename(columns={'Random_Forests':f'no_max_depth'}, inplace=True)\n",
    "RandomForest_Depth = pd.concat([RandomForest_Depth, no_max_depth], axis=1)\n",
    "RandomForest_Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataframe\n",
    "df_all = pd.get_dummies(df)\n",
    "y = df[\"binary_outcome\"]\n",
    "X = df_all.drop([\"binary_outcome_1.0\",\"binary_outcome_0.0\"],1)\n",
    "\n",
    "# initialised empty daraframe\n",
    "RandomForest_Estimators = pd.DataFrame()\n",
    "\n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "\n",
    "for item in n_estimators:\n",
    "    df1 = cross_val_RandomForest_DF(X,y,estimators=item)\n",
    "    df1.rename(columns={'Random_Forests':f'estimators={item}'}, inplace=True)\n",
    "    RandomForest_Estimators = pd.concat([RandomForest_Estimators, df1], axis=1)\n",
    "\n",
    "RandomForest_Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataframe\n",
    "df_all = pd.get_dummies(df)\n",
    "y = df[\"binary_outcome\"]\n",
    "X = df_all.drop([\"binary_outcome_1.0\",\"binary_outcome_0.0\"],1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1 , test_size=0.3)\n",
    "\n",
    "# Train RF with 100 trees\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)\n",
    "rfc.fit(X_train, y_train)\n",
    "importance = pd.DataFrame({'feature': X_train.columns, 'importance':rfc.feature_importances_})\n",
    "importance.sort_values('importance', ascending=False, inplace=True)\n",
    "importance.set_index('feature',1, inplace=True)\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up empty list to store features\n",
    "list_features = []\n",
    "\n",
    "# set up empty dataframe to store results\n",
    "RandomForest_Features = pd.DataFrame()\n",
    "count=0\n",
    "\n",
    "# loop over 'importance' dataframe adding 1 feature per loop (will be the next most important feature)\n",
    "# each loop calculates the score for the current number of features in the list_features\n",
    "# Each loop appends result to RandomForest_Features dataframe\n",
    "for index, row in importance.iterrows():\n",
    "    # only calculate the top 22 features to keep runtime down\n",
    "    if count < 25:\n",
    "        list_features.append(index)\n",
    "        X = df_all[list_features]\n",
    "        df1 = cross_val_RandomForest_DF(X,y, depth=6)\n",
    "        df1.rename(columns={'Random_Forests':f'features={len(list_features)}'}, inplace=True)\n",
    "        RandomForest_Features = pd.concat([RandomForest_Features, df1], axis=1)\n",
    "        count+=1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# to display all columns    \n",
    "#pd.set_option('display.max_columns', 40)\n",
    "RandomForest_Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set list of optimum features\n",
    "optimum_features = list_features[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimum_features = pd.get_dummies(df)\n",
    "\n",
    "\n",
    "df_all = pd.get_dummies(df)\n",
    "y = df[\"binary_outcome\"]\n",
    "X = df_all.drop([\"binary_outcome_1.0\",\"binary_outcome_0.0\"],1)\n",
    "df_optimum_features = df_all.drop([\"binary_outcome_1.0\",\"binary_outcome_0.0\"],1)\n",
    "\n",
    "X = df_optimum_features[optimum_features]\n",
    "print(\"Number of features: \", len(X.columns))\n",
    "\n",
    "# calculate Linear regression, logistic regression, random forest results again\n",
    "linRegDF = cross_val_LinReg_DF(X,y)\n",
    "logRegDF = cross_val_LogReg_DF(X,y)\n",
    "RandomForestDF = cross_val_RandomForest_DF(X,y,depth=10,estimators=64)\n",
    "# merge all 3 models into dataframe\n",
    "ResultsDF_optimum_features = pd.concat([linRegDF, logRegDF, RandomForestDF], axis=1)\n",
    "ResultsDF_optimum_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
